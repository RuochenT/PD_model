{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the PD Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating PD of individual accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "# Sets the pandas dataframe options to display all columns/ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_categories = pd.DataFrame(ref_categories, columns = ['Feature name'])\n",
    "# create a new dataframe with one column. Its values are the values from the 'reference_categories' list.\n",
    "df_ref_categories['Coefficients'] = 0\n",
    "# create a second column, called 'Coefficients', which contains only 0 values.\n",
    "df_ref_categories['p_values'] = np.nan\n",
    "# create a third column, called 'p_values', with contains only NaN values.\n",
    "df_ref_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard = pd.concat([summary_table, df_ref_categories])\n",
    "# Concatenates two dataframes.\n",
    "df_scorecard = df_scorecard.reset_index()\n",
    "#  reset the index of a dataframe.\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Original feature name'] = df_scorecard['Feature name'].str.split(':').str[0]\n",
    "# create a new column, called 'Original feature name', which contains the value of the 'Feature name' column,\n",
    "# up to the column symbol.\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = 300\n",
    "max_score = 850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard.groupby('Original feature name')['Coefficients'].min()\n",
    "# Groups the data by the values of the 'Original feature name' column.\n",
    "# Aggregates the data in the 'Coefficients' column, calculating their minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].min().sum()\n",
    "# Up to the 'min()' method everything is the same as in te line above.\n",
    "min_sum_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_scorecard.groupby('Original feature name')['Coefficients'].max()\n",
    "# Groups the data by the values of the 'Original feature name' column.\n",
    "# Aggregates the data in the 'Coefficients' column, calculating their maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].max().sum()\n",
    "# Up to the 'min()' method everything is the same as in te line above.\n",
    "# Then, we aggregate further and sum all the maximum values.\n",
    "max_sum_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Score - Calculation'] = df_scorecard['Coefficients'] * (max_score - min_score) / (max_sum_coef - min_sum_coef)\n",
    "# We multiply the value of the 'Coefficients' column by the ration of the differences between\n",
    "# maximum score and minimum score and maximum sum of coefficients and minimum sum of cefficients.\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Score - Calculation'][0] = ((df_scorecard['Coefficients'][0] - min_sum_coef) / (max_sum_coef - min_sum_coef)) * (max_score - min_score) + min_score\n",
    "# We divide the difference of the value of the 'Coefficients' column and the minimum sum of coefficients by\n",
    "# the difference of the maximum sum of coefficients and the minimum sum of coefficients.\n",
    "# Then, we multiply that by the difference between the maximum score and the minimum score.\n",
    "# Then, we add minimum score. \n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Score - Preliminary'] = df_scorecard['Score - Calculation'].round()\n",
    "# We round the values of the 'Score - Calculation' column.\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].min().sum()\n",
    "# Groups the data by the values of the 'Original feature name' column.\n",
    "# Aggregates the data in the 'Coefficients' column, calculating their minimum.\n",
    "# Sums all minimum values.\n",
    "min_sum_score_prel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].max().sum()\n",
    "# Groups the data by the values of the 'Original feature name' column.\n",
    "# Aggregates the data in the 'Coefficients' column, calculating their maximum.\n",
    "# Sums all maximum values.\n",
    "max_sum_score_prel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Difference'] = df_scorecard['Score - Preliminary'] - df_scorecard['Score - Calculation']\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Score - Final'] = df_scorecard['Score - Preliminary']\n",
    "df_scorecard['Score - Final'][77] = 16\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Final'].min().sum()\n",
    "# Groups the data by the values of the 'Original feature name' column.\n",
    "# Aggregates the data in the 'Coefficients' column, calculating their minimum.\n",
    "# Sums all minimum values.\n",
    "min_sum_score_prel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Final'].max().sum()\n",
    "# Groups the data by the values of the 'Original feature name' column.\n",
    "# Aggregates the data in the 'Coefficients' column, calculating their maximum.\n",
    "# Sums all maximum values.\n",
    "max_sum_score_prel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caclulating Credit Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat_w_intercept = inputs_test_with_ref_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat_w_intercept.insert(0, 'Intercept', 1)\n",
    "# We insert a column in the dataframe, with an index of 0, that is, in the beginning of the dataframe.\n",
    "# The name of that column is 'Intercept', and its values are 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat_w_intercept.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat_w_intercept = inputs_test_with_ref_cat_w_intercept[df_scorecard['Feature name'].values]\n",
    "# Here, from the 'inputs_test_with_ref_cat_w_intercept' dataframe, we keep only the columns with column names,\n",
    "# exactly equal to the row values of the 'Feature name' column from the 'df_scorecard' dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat_w_intercept.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard_scores = df_scorecard['Score - Final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat_w_intercept.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard_scores = scorecard_scores.values.reshape(102, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = inputs_test_with_ref_cat_w_intercept.dot(scorecard_scores)\n",
    "# Here we multiply the values of each row of the dataframe by the values of each column of the variable,\n",
    "# which is an argument of the 'dot' method, and sum them. It's essentially the sum of the products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Credit Score to PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_coef_from_score = ((y_scores - min_score) / (max_score - min_score)) * (max_sum_coef - min_sum_coef) + min_sum_coef\n",
    "# We divide the difference between the scores and the minimum score by\n",
    "# the difference between the maximum score and the minimum score.\n",
    "# Then, we multiply that by the difference between the maximum sum of coefficients and the minimum sum of coefficients.\n",
    "# Then, we add the minimum sum of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_proba_from_score = np.exp(sum_coef_from_score) / (np.exp(sum_coef_from_score) + 1)\n",
    "# Here we divide an exponent raised to sum of coefficients from score by\n",
    "# an exponent raised to sum of coefficients from score plus one.\n",
    "y_hat_proba_from_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba[0: 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs['y_hat_test_proba'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Cut-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  need the confusion matrix again.\n",
    "tr = 0.9\n",
    "df_actual_predicted_probs['y_hat_test'] = np.where(df_actual_predicted_probs['y_hat_test_proba'] > tr, 1, 0)\n",
    "#df_actual_predicted_probs['loan_data_targets_test'] == np.where(df_actual_predicted_probs['y_hat_test_proba'] >= tr, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr)\n",
    "plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs = pd.concat([pd.DataFrame(thresholds), pd.DataFrame(fpr), pd.DataFrame(tpr)], axis = 1)\n",
    "# We concatenate 3 dataframes along the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.columns = ['thresholds', 'fpr', 'tpr']\n",
    "# We name the columns of the dataframe 'thresholds', 'fpr', and 'tpr'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs['thresholds'][0] = 1 - 1 / np.power(10, 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs['Score'] = ((np.log(df_cutoffs['thresholds'] / (1 - df_cutoffs['thresholds'])) - min_sum_coef) * ((max_score - min_score) / (max_sum_coef - min_sum_coef)) + min_score).round()\n",
    "# The score corresponsing to each threshold equals:\n",
    "# The the difference between the natural logarithm of the ratio of the threshold and 1 minus the threshold and\n",
    "# the minimum sum of coefficients\n",
    "# multiplied by\n",
    "# the sum of the minimum score and the ratio of the difference between the maximum score and minimum score and \n",
    "# the difference between the maximum sum of coefficients and the minimum sum of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs['Score'][0] = max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function called 'n_approved' which assigns a value of 1 if a predicted probability\n",
    "# is greater than the parameter p, which is a threshold, and a value of 0, if it is not.\n",
    "# Then it sums the column.\n",
    "# Thus, if given any percentage values, the function will return\n",
    "# the number of rows wih estimated probabilites greater than the threshold. \n",
    "def n_approved(p):\n",
    "    return np.where(df_actual_predicted_probs['y_hat_test_proba'] >= p, 1, 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs['N Approved'] = df_cutoffs['thresholds'].apply(n_approved)\n",
    "# Assuming that all credit applications above a given probability of being 'good' will be approved,\n",
    "# when we apply the 'n_approved' function to a threshold, it will return the number of approved applications.\n",
    "# Thus, here we calculate the number of approved appliations for al thresholds.\n",
    "df_cutoffs['N Rejected'] = df_actual_predicted_probs['y_hat_test_proba'].shape[0] - df_cutoffs['N Approved']\n",
    "# Then, we calculate the number of rejected applications for each threshold.\n",
    "# It is the difference between the total number of applications and the approved applications for that threshold.\n",
    "df_cutoffs['Approval Rate'] = df_cutoffs['N Approved'] / df_actual_predicted_probs['y_hat_test_proba'].shape[0]\n",
    "# Approval rate equalts the ratio of the approved applications and all applications.\n",
    "df_cutoffs['Rejection Rate'] = 1 - df_cutoffs['Approval Rate']\n",
    "# Rejection rate equals one minus approval rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.iloc[5000: 6200, ]\n",
    "# Here we display the dataframe with cutoffs form line with index 5000 to line with index 6200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.iloc[1000: 2000, ]\n",
    "# Here we display the dataframe with cutoffs form line with index 1000 to line with index 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train_with_ref_cat.to_csv('inputs_train_with_ref_cat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard.to_csv('df_scorecard.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
